---
layout: default
---

Text can be **bold**, _italic_, or ~~strikethrough~~.

### Bio

Jonghyun Bae ~~is~~was a Ph.D student in the Department of Computer Science
and Engineering at Seoul National University. He received an M.S in Electronic,
Electrical, and Computer Engineering in 2017. and B.S in Semiconductor Systems
Engineering in 2015 -- both from Sungkyunkwan University. His current research
interests include high-performance training of DNNs and high-throughput big data analytics.

---

### Education

*Seoul National University*   
Ph.D. in Computer Science and Engineering (Sep 2017 -- Feb 2022)   
Advisor: [Jae W. Lee](https://iamjaelee.github.io/www/)   
Dissertation: A Large-Batch, High-Throughput Training System for Deep Neural Networks

*Sungkyunkwan University*   
M.S. in Electronic, Electrical and Computer Engineering (Mar 2015--Aug 2017)   
Advisor: [Jae W. Lee](https://iamjaelee.github.io/www/) and [Jaehyuk Choi](https://sites.google.com/view/eemix/people#h.p_ID_53)
Dissertation: Jointly Optimizing Task Granularity and Concurrency for In-Memory MapReduce Frameworks

<span style="font-weight:500">Sungkyunkwan University</span> - B.S. in Semiconductor Systems Engineering (Mar 2009--Feb 2015)

---

### International Conferences
<b>[USENIX ATC '21]</b>
<span style="font-weight:500">ASAP: Fast Mobile Application Switch via Adaptive Prepaging</span><br>
Sam Son, Seung Yul Lee, Yunho Jin, <b>Jonghyun Bae</b>, Jinkyu Jeong, Tae Jun Ham, Jae W. Lee, and Hongil Yoon<br>
<i>USENIX Annual Technical Conference (ATC), Virtual, 2021</i>
<i>Acceptance rate: 64/341=23(%) </i><br>


<b>[FAST '21]</b>
<span style="font-weight:500">Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs</span><br>
Shine Kim\*, Yunho Jin\*, Gina Sohn, <b>Jonghyun Bae</b>, Tae Jun Ham, and Jae W. Lee<br>
<i>USENIX Conference on File and Storage Technologies (FAST), Virtual, 2021</i><br>
<i>Acceptance rate: 28/130=21(%) </i><br>
\* Equal Contributions<br>


<b>[FAST '21]</b>
<span style="font-weight:500">FlashNeuron: SSD-Enabled Large-Batch Training of Very Deep Neural Networks</span><br>
<b>Jonghyun Bae</b>, Jongsung Lee, Yunho Jin, Sam Son, Shine Kim, Hakbeom Jang, Tae Jun Ham, and Jae W. Lee<br>
<i>USENIX Conference on File and Storage Technologies (FAST), Virtual, 2021</i><br>
<i>Acceptance rate: 28/130=21(%) </i><br>


<b>[HPCA '21]</b>
<span style="font-weight:500">Layerweaver: Maximizing Resource Utilization of Neural Processing Units via Layer-Wise Scheduling</span><br>
Young H. Oh, Seonghak Kim, Yunho Jin, Sam Son, <b>Jonghyun Bae</b>, Jongsung Lee, Yeonhong Park, Dong Uk Kim, Tae Jun Ham, and Jae W. Lee<br>
<i>IEEE International Symposium on High Performance Computer Architecture (HPCA), Seoul, Korea, 2021</i><br>
<i>Acceptance rate: 63/258=24(%) </i><br>


<b>[ISCA '20]</b>
<span style="font-weight:500">A Case for Hardware-based Demand Paging</span><br>
Gyusun Lee\*, Wenjing Jin\*, Wonsuk Song, Jeonghun Gong, <b>Jonghyun Bae</b>, Tae Jun Ham, Jae W. Lee, and Jinkyu Jeong<br>
<i>ACM/IEEE International Symposium on Computer Architecture (ISCA), Valencia, Spain, 2020</i><br>
<i>Acceptance rate: 77/428=18(%) </i><br>
\* Equal Contributions<br>


<b>[USENIX ATC '19]</b>
<span style="font-weight:500">Practical Erase Suspension for Modern Low-latency SSD</span><br>
Shine Kim, <b>Jonghyun Bae</b>, Hakbeom Jang, Wenjing Jin, Jeonghun Gong, Seungyeon Lee, Tae Jun Ham, and Jae W. Lee<br>
<i>USENIX Annual Technical Conference (ATC), Renton, WA, 2019</i>


<b>[BigData '17]</b>
<span style="font-weight:500">Jointly Optimizing Task Granularity and Concurrency for In-Memory MapReduce Frameworks</span><br>
<b>Jonghyun Bae</b>\*, Hakbeom Jang\*, Wenjing Jin, Jun Heo, Jaeyoung Jang, Joo-Young Hwang, Sangyeun Cho, and Jae W. Lee<br>
<i>IEEE International Conference on Big Data (BigData), Boston, MA, December 2017</i><br>
\* Equal Contributions<br>


<b>[UseR!-Poster '14]</b>
<span style="font-weight:500">RIGHT: An HTML Canvas and JavaScript-based Interactive Data Visualization Package for Linked Graphics</span><br>
ChungHa Sung, <u>JongHyun Bae</u>, SangGi Hong, TaeJoon Song, Jae W. Lee, and Junghoon Lee<br>
<i>The R User Conference, Los Angeles, CA, July 2014</i>



### International Journals
<b>[IEEE Micro '19]</b>
<span style="font-weight:500">SSDStreamer: Specializing I/O Stack for Large-Scale Machine Learning</span><br>
<b>Jonghyun Bae</b>, Hakbeom Jang, Jeonghun Gong, Wenjing Jin, Shine Kim, Jaeyoung Jang, Tae Jun Ham, Jinkyu Jeong, and Jae W. Lee <br>
<i>IEEE Micro, Sept/Oct 2019</i>


<b>[IEICE TIS '19]</b>
<span style="font-weight:500">Eager Memory Managerment for In-Memory Data Analytics</span><br>
Hakbeom Jang, <b>Jonghyun Bae</b>, Tae Jun Ham, and Jae W. Lee<br>
<i>IEICE Transactions on Information and Systems, 2018</i>



### International Workshops
<b>[Spark Summit East '16]</b>
<span style="font-weight:500">ggplot2.SparkR: Rebooting ggplot2 for Scalable Big Data Visualization</span><br>
<b>Jonghyun Bae</b>, Sangoh Jeong, Wenjing Jin, and Jae W. Lee<br>
<i>Spark Summit East, New York City, New York, February 2016</i>

---

### Experience

<span style="font-weight:600">NAVER Clova AI Research Intern</span><br>
May 2020 - August 2020<br>
<span style="font-weight:500">Research on automatically searches for effective augmentation policies</span>

<span style="font-weight:600">Google Summer of Code</span><br>
March 2014 - August 2014<br>
<span style="font-weight:500">Improving the R-interactive-Graphics-via-HTml (RIGHT) Package</span>

